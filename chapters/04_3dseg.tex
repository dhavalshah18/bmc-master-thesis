\section{Prior Networks for Aneurysm Segmentation}
\label{chapter4}

Two methods of segmentation of intracranial aneurysms using 3D neural networks are evaluated with the dataset obtained from the ADAM challenge. This includes the adapted DeepMedic framework proposed by \citeauthor{Sichermann2019} and initially by \citeauthor{Kamnitsas2017}, and the no-new-Net (nnU-Net) architecture proposed by \citeauthor{nnUnet} which was adapted for the ADAM challenge and received first place. 

These networks are chosen firstly due to their performance; \citeauthor{Sichermann2019} showed a good DSC with their adapted framework in their study, and the DeepMedic framework also generally performs well in various other domains, thus making it an interesting addition for comparison, and finally the nnU-Net can be regarded as the gold standard architecture due to its peak performance in the challenge -- for the same dataset used in this study.

In the following sections the architectures used are described and shown along with the experiments run in the study, and how these architectures will be adapted to this case (if any adaptation is needed). 

%\section{3D U-Net}
%The 3D-Unet architecture consists of an analysis and synthesis path -- also referred to as encoder and decoder paths -- with four steps, each with different resolutions. Each layer in the analysis path contains two convolutions with a kernel size of 3 followed by a rectified linear unit (ReLU), followed by a max pooling layer with a kernel size of 2 and a stride of 2. Many variations have been proposed to the standard architecture, such as the use of residual connections, attention mechanisms, or dilated convolutions, however as a baseline it can be hypothesized that the standard architecture should perform adequately for this task. Figure \ref{fig:unet.png} illustrates the network architecture.
%
%\img{unet.png}{\linewidth}{\todo{Caption}}{\todo{Small caption}}
%
%\todo{Go into training, benefits, prior uses of network. Also how it forms basis of nnUnet framework}


\subsection{DeepMedic}
The network proposed by \citeauthor{Sichermann2019} was initially devised by \citeauthor{Kamnitsas2017}; it is a dual pathway, 11-layers deep, 3D Convolutional Neural Network (CNN) originally meant for the task of brain lesion segmentation. Both pathways of the network are identical, however the inputs to the second pathway are downsampled versions of the images in the first pathway. The outputs of both pathways are concatenated before the fully connected layers and finally classified. The full pipeline is shown in Figure \ref{fig:deep-medic.jpg}.

\img{deep-medic.jpg}{\linewidth}{The neural network DeepMedic, with a 2-pathway architecture. Each layer shows the number of feature maps and their size as number $\times$ size. ReLU activation and batch normalization are also applied after each layer. The diagram is taken from \citeauthor{Sichermann2019}.}{DeepMedic architecture}

Initially, \citeauthor{Kamnitsas2017} proposed DeepMedic to efficiently incorporate both local and contextual information by using the parallel, multi-scale pathways. They also employ a fully-connected Conditional Random Field (CRF) model for final post-processing of the segmentation maps \cite{Krahenbuhl2012}. However, \citeauthor{Sichermann2019} chose to exclude that in favor of thresholding solely based on aneurysm size. Also, in the study, DeepMedic was trained solely on cases that contained intracranial aneurysms, and thus the network could prove to have worse performance on the current dataset. Aneurysms in their dataset also had a mean diameter of $7.10$ mm, much larger than the $4.11$ mm average in the current dataset, which could also further deteriorate performance. The DeepMedic framework was initially designed taking into consideration resource usage and therefore it could prove to require less parameters compared to the network discussed in the following section. In the study, their results showed a high false-positive rate which could further increase given our dataset contains aneurysms of a much smaller size.

DeepMedic was trained for this task as described in the paper by \citeauthor{Sichermann2019}; with a learning rate of $10^{-4}$, optimization with Adam and a Nesterov momentum of $0.6$, and a hybrid training scheme in which $90\%$ of input image segments correspond to the background class and $10\%$ to the foreground class. For training, dice loss was used which takes the form:

 \[L = 1 - \frac{2\sum_{i}^{N}p_{i}g_{i}}{\sum_{i}^{N}p_{i}^{2} + \sum_{i}^{N}g_{i}^{2}} \]

where $p_{i}$ and $g_{i}$ represent the voxel values of the predicted binary segmentation and the ground truth binary volume, summed over $N$ voxels \cite{milletari2016v}. They considered and evaluated various preprocessing steps for the best output, however since the other two network models to be evaluated will use only one set of images, this will be the same data used for this model. Thus, the data used will be as stated under \ref{chapter3}. The Tensorflow framework was used on an NVIDIA-P100 GPU with 16 GB memory for both training and inference. 

\subsection{nnU-Net}
The ``no-new-Net'' was presented in \citeyear{nnUnet}, and it is a self-adapting framework on the basis of vanilla U-Nets. It automatically configures itself including preprocessing, network architecture, training and post-processing for any new task in the biomedical domain. The framework does not introduce a new network architecture (hence the name ``no-new-net'') but rather aims to systematize the process of manual method configuration.

The automatic design of the framework contains three parameter groups: fixed, rule-based and empirical parameters. First, all design choices that do not require adaptation between datasets are collected and their joint configuration optimized -- for example, setting the architecture template to ``U-Net-like''. Second, explicit dependencies are formed between the dataset fingerprint (standardized dataset representation comprising of key properties) and the pipeline fingerprint (choices made during method design). These dependencies are modeled in the form of interdependent heuristic rules. Third, the remaining design choices are set and decided upon empirically based on training data. This forms the basis of the development of the nnU-Net. Figure \ref{fig:nnunet.png} shows the three parameter groups, as well as some specific aspects that go into each.

\img{nnunet.png}{\linewidth}{Proposed automated method configuration for the nnU-Net framework for biomedical image segmentation tasks, adapted from \cite{nnUnet}. Dataset properties are extracted from the specific train data for the task to form a dataset fingerprint. Rule-based parameters of the pipeline are inferred based on the data fingerprint using a set of heuristic rules, and the fixed parameters are predefined .}{Automated method configuration of nnU-Net framework.}

During application of the nnU-Net, despite the few empirical choices to be made, the automatic configuration runs without manual intervention, and thus no additional computation cost beside that of the standard network training procedure is required. The automatic method configuration is initiated with the extraction of the dataset fingerprint followed by execution of the heuristic rules. Three U-Net configurations are generated by default: a 2D U-Net, a 3D U-Net operating on full resolution images, and a 3D U-Net cascade -- where the former U-Net operates on downsampled images, and the latter on full resolution images used to refine the segmentation maps created by the former. The best configuration or ensemble is chosen after cross-validation. 

Although the exact data fingerprint and pipeline fingerprint for this dataset cannot be extracted, the key properties taken into consideration for the task of UIA segmentation could be hypothesized. From the dataset the distribution of spacings and intensity distribution would be important factors for this modality -- due to the sparcity of positive labels in all images, the variance in voxel spacing across images and the changing intensity distribution across cases, and thus the rule based parameters such as the annotation and image resampling strategy, patch size, intensity normalization would be explicitly defined from the data fingerprint. These heuristic rules would be used to automatically infer the choices of architecture design -- i.e. the pipeline fingerprint -- so as to reduce any manual design choices that need to be made.

Training of the nnU-Net was not performed, as a pre-trained network was made available from \href{https://github.com/JunMa11/ADAM2020/}{https://github.com/JunMa11/ADAM2020/}, which is the submission made to the ADAM challenge -- i.e. it was trained for the same task and with the same dataset. From the submission made to the challenge: to mitigate the highly imbalanced dataset with respect to foreground and background pixels, a combination of dice loss and cross-entropy loss is used for training of the network, which takes the form:

\[L = 1 - \frac{2\sum_{i=1}^{N}p_{i}g_{i}}{\sum_{i=1}^{N}p_{i}^{2} + \sum_{i=1}^{N}g_{i}^{2}} - \frac{1}{N}\sum_{i=1}^{N}g_{i}\log p_{i}\]

where $p_{i}$ and $g_{i}$ represent the voxel values of the predicted binary segmentation and the ground truth binary volume, summed over $N$ voxels \cite{milletari2016v}. The network was trained using Stochastic Gradient Descent (SGD) with momentum, a patch size of $56 \times 256 \times 224$ and five-fold cross validation on a NVIDIA Titan V100 GPU. It is unclear if the data used was preprocessed. 

The nnU-Net framework is able to handle an expansive variety of datasets and target image properties; heart segmentation in CT images, liver, spleen, left and right kidneys in T1 in-phase MRI images, abdominal organs in CT images are just some examples of different test sets to which the network has successfully been employed. In the case of the ADAM challenge it was the winning submission with the results as shown in Chapter \ref{chapter6}. The focus with the framework is to automatically adapt to any new dataset with minimum configuration, and thus it is able to achieve state-of-the-art (and better) results. However, no importance is given to resource consumption in this regard, and with the proposed network the goal is to achieve similar performance with much less required parameters.


